library(tidyr)
library(stringr)
library(dplyr)
library(mice)
library(corrplot)
library(VIM)
library(psych)
library(ggplot2)
library(cluster)
library(mclust)

Data = read.csv("C:\\Users\\rafae\\Documents\\AulasS1\\Reconhecimento de padrões\\trabalho rp\\CSVPadroesv2.csv", header = TRUE)

Data <- head(Data, -5)

#Coloca a DB com dados por cada país
Data <- Data %>%
  select(-Series.Code) %>%
  spread(key = Series.Name, value = X2015..YR2015.) %>%
  mutate_all(~na_if(.,".."))

names(Data) <- gsub(" ", ".", names(Data))
names(Data) <- gsub(",", "", names(Data))
names(Data) <- gsub("\\(", "", names(Data))
names(Data) <- gsub("\\$", "D", names(Data))
names(Data) <- gsub("\\)", "", names(Data))
names(Data) <- gsub("\\%", "", names(Data))

#Verifica os indices que têm mais de 20% de Nulos
prop_na <- colMeans(is.na(Data))
print(prop_na)
colunas_manter <- names(prop_na[prop_na <= 0.2])
colunas_manter

#df
prop_na_df <- data.frame(
  Variavel = names(prop_na),
  Proporcao_NA = prop_na
)
prop_na_df <- prop_na_df %>%
  slice(3:n())

#Gráfico de barras para analisar NAs
# Criar gráfico de barras
ggplot(prop_na_df, aes(x = Variavel, y = Proporcao_NA, fill = Variavel)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Proporcao_NA, 2)), vjust = -0.5) +  # Adiciona texto com as proporções
  labs(title = "Proporção de Valores Ausentes por Variável", x = "Variável", y = "Proporção de NA") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 70, hjust = 1)) +
  guides(fill = FALSE)


#Retira os indices com mais de 20% de nulos
Data <- Data[colunas_manter]

#Verifica os paises que têm mais de 20% de Nulos
prop_na_lines <- rowMeans(is.na(Data))
print(prop_na_lines)
prop_na_lines_df = data.frame(Pais = Data$Country.Name , Percentagem = prop_na_lines)
paises_manter <- prop_na_lines_df %>%
  filter(Percentagem < 0.2) %>%
  pull(Pais)

#Países a excluir
paises_excluir_df <- prop_na_lines_df %>%
  filter(Percentagem > 0.2)
#Plot
ggplot(paises_excluir_df, aes(x = Pais, y = Percentagem, fill = Pais)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Percentagem, 2)), vjust = -0.5) +  # Adiciona texto com as proporções
  labs(title = "Proporção de Valores Ausentes por País", x = "País", y = "Proporção de NA") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 70, hjust = 1)) +
  guides(fill = FALSE)


#Retira os paises com mais de 20% de nulos
Data <- Data %>%
  filter(Country.Name %in% paises_manter)

#Ajustar colunas para numeric 
{
  Data$Adjusted.net.national.income.per.capita.current.USD <- as.numeric(Data$Adjusted.net.national.income.per.capita.current.USD)
  Data$Birth.rate.crude.per.1000.people <- as.numeric(Data$Birth.rate.crude.per.1000.people)
  Data$Current.health.expenditure..of.GDP <- as.numeric(Data$Current.health.expenditure..of.GDP)
  Data$Foreign.direct.investment.net.BoP.current.USD <- as.numeric(Data$Foreign.direct.investment.net.BoP.current.USD)
  Data$GDP.current.USD <- as.numeric(Data$GDP.current.USD)
  Data$GDP.per.capita.current.USD <- as.numeric(Data$GDP.per.capita.current.USD)
  Data$Individuals.using.the.Internet..of.population <- as.numeric(Data$Individuals.using.the.Internet..of.population)
  Data$Inflation.GDP.deflator.annual. <- as.numeric(Data$Inflation.GDP.deflator.annual.)
  Data$Labor.force.total <- as.numeric(Data$Labor.force.total)
  Data$Life.expectancy.at.birth.total.years <- as.numeric(Data$Life.expectancy.at.birth.total.years)
  Data$Mortality.rate.infant.per.1000.live.births <- as.numeric(Data$Mortality.rate.infant.per.1000.live.births)
  Data$People.using.at.least.basic.drinking.water.services..of.population <- as.numeric(Data$People.using.at.least.basic.drinking.water.services..of.population)
  Data$Population.total <- as.numeric(Data$Population.total)
  Data$School.enrollment.primary..gross <- as.numeric(Data$School.enrollment.primary..gross)
  Data$Suicide.mortality.rate.per.100000.population <- as.numeric(Data$Suicide.mortality.rate.per.100000.population)
  }
# Conversão colunas de percentagem para decimais
Data$Current.health.expenditure..of.GDP <- Data$Current.health.expenditure..of.GDP / 100
Data$Individuals.using.the.Internet..of.population <- Data$Individuals.using.the.Internet..of.population / 100
Data$People.using.at.least.basic.drinking.water.services..of.population <- Data$People.using.at.least.basic.drinking.water.services..of.population / 100

### Análise exploratótia por variável
#Summary
summary(Data$Adjusted.net.national.income.per.capita.current.USD)
summary(Data$Birth.rate.crude.per.1000.people)
summary(Data$Current.health.expenditure..of.GDP)
summary(Data$Foreign.direct.investment.net.BoP.current.USD)
summary(Data$GDP.current.USD)
summary(Data$GDP.per.capita.current.USD)
summary(Data$Individuals.using.the.Internet..of.population)
summary(Data$Inflation.GDP.deflator.annual.)
summary(Data$Labor.force.total)
summary(Data$Life.expectancy.at.birth.total.years)
summary(Data$Mortality.rate.infant.per.1000.live.births)
summary(Data$People.using.at.least.basic.drinking.water.services..of.population)
summary(Data$Population.total)
summary(Data$School.enrollment.primary..gross)
summary(Data$Suicide.mortality.rate.per.100000.population)

#Boxplots
for (col in names(Data)) {
  if (is.numeric(Data[[col]])) {
    boxplot(Data[[col]], main = paste("Boxplot de", col), col = "lightblue", horizontal = TRUE)
  }
}

# Encontra os Outliers e substitui por NA
for (col in names(Data)) {
  if (is.numeric(Data[[col]])) {
    quartiles <- quantile(Data[[col]], probs=c(.25, .75), na.rm = TRUE)
    
    iQRange <- IQR(Data[[col]], na.rm = TRUE)
    lowerLimit <- quartiles[1] - 1.5 * iQRange
    upperLimit <- quartiles[2] + 1.5 * iQRange
    
    # Substituir outliers por NA diretamente na coluna existente
    Data[[col]][Data[[col]] < lowerLimit | Data[[col]] > upperLimit] <- NA
  }
}

str(Data)

#Realizar a imputação KNN 
Data_kNN <- kNN(Data, k = 5, imp_var = FALSE)

###Normalizar###
# Identificar as colunas numéricas
colunas_numericas <- sapply(Data_kNN, is.numeric)

# Normalizar manualmente todas as colunas numéricas para o intervalo [0, 1] (método Min-Max)
normalize_minmax <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Aplicar a função de normalização a todas as colunas numéricas
Data_kNN[, colunas_numericas] <- lapply(Data_kNN[, colunas_numericas], normalize_minmax)

## Alteração do nome das colunas por siglas para facilitar a visualização na matriz de correlação
#Ver nomes
names(Data_kNN)

#Criar dicionário para consulta
Dicionario <- data.frame(
  Nomes = c("Adjusted.net.national.income.per.capita.current.USD","Birth.rate.crude.per.1000.people","Current.health.expenditure.%.of.GDP","Foreign.direct.investment.net.BoP.current.USD","GDP.current.USD","GDP.per.capita.current.USD","Individuals.using.the.Internet.%.of.population","Inflation.GDP.deflator.annual.","Labor.force.total","Life.expectancy.at.birth.total.years","Mortality.rate.infant.per.1000.live.births","People.using.at.least.basic.drinking.water.services.%.of.population","Population.total","School.enrollment.primary..gross","Suicide.mortality.rate.per.100000.population"),
  Siglas = c("ANNIPC","BR","HE","FDI","GDP","GDPPC","IUI","IGDA","LF","LEAB","MRI","BDWS","POP","SEP","SMR")
)

#Alterar os nomes para as Siglas assignadas no dicionário
{
colnames(Data_kNN)[colnames(Data_kNN) == "Adjusted.net.national.income.per.capita.current.USD"] <- "ANNIPC"
colnames(Data_kNN)[colnames(Data_kNN) == "Birth.rate.crude.per.1000.people"] <- "BR"
colnames(Data_kNN)[colnames(Data_kNN) == "Current.health.expenditure..of.GDP"] <- "HE"
colnames(Data_kNN)[colnames(Data_kNN) == "Foreign.direct.investment.net.BoP.current.USD"] <- "FDI"
colnames(Data_kNN)[colnames(Data_kNN) == "GDP.current.USD"] <- "GDP"
colnames(Data_kNN)[colnames(Data_kNN) == "GDP.per.capita.current.USD"] <- "GDPPC"
colnames(Data_kNN)[colnames(Data_kNN) == "Individuals.using.the.Internet..of.population"] <- "IUI"
colnames(Data_kNN)[colnames(Data_kNN) == "Inflation.GDP.deflator.annual."] <- "IGDA"
colnames(Data_kNN)[colnames(Data_kNN) == "Labor.force.total"] <- "LF"
colnames(Data_kNN)[colnames(Data_kNN) == "Life.expectancy.at.birth.total.years"] <- "LEAB"
colnames(Data_kNN)[colnames(Data_kNN) == "Mortality.rate.infant.per.1000.live.births"] <- "MRI"
colnames(Data_kNN)[colnames(Data_kNN) == "People.using.at.least.basic.drinking.water.services..of.population"] <- "BDWS"
colnames(Data_kNN)[colnames(Data_kNN) == "Population.total"] <- "POP"
colnames(Data_kNN)[colnames(Data_kNN) == "School.enrollment.primary..gross"] <- "SEP"
colnames(Data_kNN)[colnames(Data_kNN) == "Suicide.mortality.rate.per.100000.population"] <- "SMR"
}


####ACP####
#Summary and covariance matrix
Data_numeric <- Data_kNN[,3:17]
summary(Data_numeric)
round(var(Data_numeric),2)

#scatterplot
pairs(Data_numeric, pch = 19, lower.panel = NULL)

# Adicionar uma legenda
legend("bottomleft", legend = colnames(Data_numeric), col = 1:ncol(Data_numeric), pch = 19, title = "Variáveis")

#correlation matrix
correlation <- cor(Data_numeric)
round(correlation,3)

#corrplot
par(oma = c(2, 2, 2, 2)) # space around for text
corrplot.mixed(correlation, 
               order = "hclust", #order of variables
               tl.pos = "lt", #text left + top
               upper = "ellipse",
               tl.cex = 0.5)

#Bartlett test - valor p baixo -> rejeita-se H0 -> há diferenças entre a matriz de correlação observada e a matriz de identidade 
cortest.bartlett(correlation)

#KMO - Overall e individual MSA com valores relativamente altos
KMO(correlation)






#### PCA´s ####





pc15 <- principal(Data_numeric, nfactors=15, rotate="none", scores=TRUE) 

#Eigenvalues - Variances of the principal components 
round(pc15$values,3)
#CRITÉRIO DE KAISER - reter os quatro primeiros componentes principais, já que são os únicos com valores maiores que 1

#Screeplot - Find the elbow
plot(pc15$values, type = "b", main = "Scree plot dos Dados", xlab = "Number of PC", ylab = "Eigenvalue")
#De acordo com este gráfico, o "cotovelo" está no 3, por isso reter as 3 primeiras componentes principais

#Olhar para a cumulative Var
pc15$loadings

#Com base nas Cumulative Var e no critério de Keiser, escolher PC4

#Let's extract a 4 component solution
pc4 <- principal(Data_numeric, nfactors=4, rotate="none")
pc4
#Let's rotate the 4 components using varimax
pc4r <- principal(Data_numeric, nfactors=4, rotate="varimax")
pc4r$loadings

#Communalities
round(pc4r$communality,4)



#Escolher nomes para cada PCA



#Compute the scores
pc4sc <- principal(Data_numeric, nfactors=4, rotate="none", scores=TRUE) 
round(pc4sc$scores,4)

#Add scores to the data set as new variables
Data_kNN$Progresso.Socioenonomico <- pc4sc$scores[,1]
Data_kNN$Dinamica.Populacional.e.Mercado.de.Trabalho <- pc4sc$scores[,2]
Data_kNN$Iniciativa.economica <- pc4sc$scores[,3]
Data_kNN$Saude.mental.e.Educacao <- pc4sc$scores[,4]

#Save the new data set in excel format
#setwd("C:\\2\\")
#write.xlsx(Data_kNN, file = "NewDecathlon.xlsx", 
           #sheetName = "PCAscores", col.names = TRUE, 
           #row.names = TRUE, append = FALSE)


#Depict the scatterplot of PC1 vs PC2
#plot(Data_kNN$pc1, Data_kNN$pc2, pch = 19,xlim = c(-2,3),
     #ylim = c(-3,2), xlab="PC1", ylab="PC2", main = "Scores: PC1 vs Pc2")
#text(Data_kNN$pc1, Data_kNN$pc2-0.1, Data_kNN[,2]) #(x,y,labels)

#Exemplo de correlação entre PC1 e uma variável qualquer
#Compute correlation: Points vs PC1 
#cor(Data_kNN$Points,Data_kNN$pc1)





#### Clusters ####




DataPCA<-Data_kNN[,c(2,18,19,20,21)] #componentes principais
#profile<-df[,c(2,14)] #variaveis profile!!!!!!!

countries<- DataPCA[,1]
DataPCA<- DataPCA[,-1]







### Clustering Hierárquico ###

##Método Ward 

demodist<-dist(DataPCA) #compute distance
hclust_demow <- hclust(demodist,method='ward.D2')
plot(hclust_demow,label = countries,hang=-1)


#cortar o dendrograma em 4 clusters
groups.k4<-cutree(hclust_demow, k=4) # cut tree into 4 clusters
rect.hclust(hclust_demow, k=4, border="red") 
aggregate(DataPCA,list(groups.k4), mean) #média de cada componente em cada um dos clusters
table(groups.k4) #composição dos clusters


#Silhouette
plot(silhouette(groups.k4,demodist)) 
#average silhouette 0.18, indica que não parece existir grande heterogeneidade
#possivelmente os clusters vão estar muito sobrepostos


#Utilizando 5 clusters
#cortar o dendrograma em 5 clusters
plot(hclust_demow,label = countries, hang=-1)
groups.k5<-cutree(hclust_demow, k=5) 
rect.hclust(hclust_demow, k=5, border="red") 
aggregate(DataPCA,list(groups.k5), mean)
table(groups.k5)

#Silhouette
plot(silhouette(groups.k5,demodist)) 
#average silhouette 0.2



## Método Complete Linkage

hclust_democ<-hclust(demodist) #default method
plot(hclust_democ,label = countries, hang=-1)
#dificil visualizaçao devido ao comportamento em cadeia



#utilizando o complete com 4 cluster
#cortar o dendrograma em 4 clusters
groups.k4c<-cutree(hclust_democ, k=4) # cut tree into 4 clusters
rect.hclust(hclust_democ, k=4, border="red") 
aggregate(DataPCA,list(groups.k4c), mean) #média de cada componente em cada um dos clusters
table(groups.k4c) #composição dos clusters
table(groups.k4,groups.k4c)


#Silhouette
plot(silhouette(groups.k4c,demodist)) 
#average silhouette 0.14


#cortar o dendrograma em 5 clusters
groups.k5c<-cutree(hclust_democ, k=5) # cut tree into 4 clusters
rect.hclust(hclust_democ, k=5, border="red") 
aggregate(DataPCA,list(groups.k5c), mean) #média de cada componente em cada um dos clusters
table(groups.k5c) #composição dos clusters
table(groups.k5,groups.k5c)


#Silhouette
plot(silhouette(groups.k5c,demodist)) 
#average silhouette 0.15

#vamos assumir o método Ward que é por convençao o melhor

#Tentou-se averiguar o porquê das diferenças entre o método Ward e o complete
#tentar perceber que músicas ficam no cluster 4 e 5 do complete 

dfc<-cbind(DataPCA, profile, groups.k5c)
summary(dfc[1:4])

dfc5<-subset(dfc, groups.k5c==5)
summary(dfc5) #observaçoes que assumem valores altos das variaveis PC2, PC3 e PC4, mas baixos de PC1


dfc4<-subset(dfc, groups.k5c==4)
summary(dfc4) #parecem ser musicas que assumem valores altos da variavel PC4 (musicas mais provaveis de ser tocadas ao vivo)












### Partitioning approach ###

set.seed(444)
#Scree plot
wssplot <- function(xx, nc=15, seed=1234){
  wss <- (nrow(DataPCA)-1)*sum(apply(DataPCA,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(xx, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}
wssplot(DataPCA, nc=10) 
#parece indicar 5 cluster, o cotovelo parece estar no 5
#vai ao encontro da opção anterior (Ward)


# K-Means (com 4 clusters)
kmeans.k4 <- kmeans(DataPCA, 4,nstart=100) 
# k = 4 from hclust, nstart = initial random solutions
attributes(kmeans.k4)  # all elements of the cluster output
kmeans.k4$centers
kmeans.k4$cluster
kmeans.k4$size
table(groups.k4,kmeans.k4$cluster)
#Silhouette
plot(silhouette(kmeans.k4$cluster,demodist))
#0.24


set.seed(444)
# K-Means (com 5 clusters)
kmeans.k5 <- kmeans(DataPCA, 5,nstart=100) 
# k = 5 from hclust, nstart = initial random solutions
attributes(kmeans.k5)  # all elements of the cluster output
kmeans.k5$centers
kmeans.k5$cluster
kmeans.k5$size
table(kmeans.k5$cluster)
table(groups.k5,kmeans.k5$cluster)

#Silhouette
plot(silhouette(kmeans.k5$cluster,demodist))
#0.25, weak structure
#no entanto,podemos usar as variáveis profilling para perceber se a estrutura é artificial ou não
#gráfico dos clusters
pairs(DataPCA, col=c(1:5)[kmeans.k5$cluster], lower.panel = NULL)












### PAM clustering ###
#It is a robust version of k-means, which clusters data around the medoids


set.seed(444)
pam.k5 <- pam(DataPCA,5)
pam.k5$medoids
pam.k5$clustering
table(kmeans.k5$cluster,pam.k5$clustering)
plot(silhouette(pam.k5$clustering,demodist)) #0.2


#gráfico dos clusters 
pairs(DataPCA, col=c(1:5)[pam.k5$clustering], lower.panel = NULL)

clusplot(pam.k5, labels = 5, col.p = pam.k5$clustering)









### Gaussian mixture model (GMM) ###

set.seed(444)
# Apply GMM with 5 components
results.k5 <- Mclust(DataPCA, G = 5)
results.k5$modelName
results.k5$classification
table(results.k5$classification)
summary(results.k5, parameters = TRUE)
table(results.k5$classification,kmeans.k5$cluster) 
pairs(DataPCA, col=c(1:5)[results.k5$classification], lower.panel = NULL)


results.k5 <- Mclust(DataPCA, G = 5)
plot(results.k5, what = "density")
plot(results.k5, what = "uncertainty")

# Model selection
BIC <- mclustBIC(DataPCA)
plot(BIC)
# Model estimation
results <- Mclust(DataPCA, x = BIC)
summary(results, parameters = TRUE)


plot(results, what = "classification")
table(class, results$classification)
plot(results, what = "uncertainty")
