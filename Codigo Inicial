library(tidyr)
library(stringr)
library(dplyr)
library(mice)
library(corrplot)
library(VIM)
library(psych)
library(ggplot2)
library(cluster)
library(mclust)
library(factoextra)

Data <- read.csv("CSVPadroesv2.csv", sep = ",", dec = ".", header = T)

Data <- head(Data, -5)

#Coloca a DB com dados por cada país
Data <- Data %>%
  select(-Series.Code) %>%
  spread(key = Series.Name, value = X2015..YR2015.) %>%
  mutate_all(~na_if(.,".."))

names(Data) <- gsub(" ", ".", names(Data))
names(Data) <- gsub(",", "", names(Data))
names(Data) <- gsub("\\(", "", names(Data))
names(Data) <- gsub("\\$", "D", names(Data))
names(Data) <- gsub("\\)", "", names(Data))
names(Data) <- gsub("\\%", "", names(Data))

#Verifica os indices que têm mais de 20% de Nulos
prop_na <- colMeans(is.na(Data))
print(prop_na)
colunas_manter <- names(prop_na[prop_na <= 0.2])
colunas_manter

#df
prop_na_df <- data.frame(
  Variavel = names(prop_na),
  Proporcao_NA = prop_na
)
prop_na_df <- prop_na_df %>%
  slice(3:n())

#Gráfico de barras para analisar NAs
# Criar gráfico de barras
ggplot(prop_na_df, aes(x = Variavel, y = Proporcao_NA, fill = Variavel)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Proporcao_NA, 2)), vjust = -0.5) +  # Adiciona texto com as proporções
  labs(title = "Proporção de Valores Ausentes por Variável", x = "Variável", y = "Proporção de NA") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 70, hjust = 1)) +
  guides(fill = FALSE)


#Retira os indices com mais de 20% de nulos
Data <- Data[colunas_manter]

#Verifica os paises que têm mais de 20% de Nulos
prop_na_lines <- rowMeans(is.na(Data))
print(prop_na_lines)
prop_na_lines_df = data.frame(Pais = Data$Country.Name , Percentagem = prop_na_lines)
paises_manter <- prop_na_lines_df %>%
  filter(Percentagem < 0.2) %>%
  pull(Pais)

#Países a excluir
paises_excluir_df <- prop_na_lines_df %>%
  filter(Percentagem > 0.2)
#Plot
ggplot(paises_excluir_df, aes(x = Pais, y = Percentagem, fill = Pais)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Percentagem, 2)), vjust = -0.5) +  # Adiciona texto com as proporções
  labs(title = "Proporção de Valores Ausentes por País", x = "País", y = "Proporção de NA") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 70, hjust = 1)) +
  guides(fill = FALSE)


#Retira os paises com mais de 20% de nulos
Data <- Data %>%
  filter(Country.Name %in% paises_manter)

#Ajustar colunas para numeric 
{
  Data$Adjusted.net.national.income.per.capita.current.USD <- as.numeric(Data$Adjusted.net.national.income.per.capita.current.USD)
  Data$Birth.rate.crude.per.1000.people <- as.numeric(Data$Birth.rate.crude.per.1000.people)
  Data$Current.health.expenditure..of.GDP <- as.numeric(Data$Current.health.expenditure..of.GDP)
  Data$Foreign.direct.investment.net.BoP.current.USD <- as.numeric(Data$Foreign.direct.investment.net.BoP.current.USD)
  Data$GDP.current.USD <- as.numeric(Data$GDP.current.USD)
  Data$GDP.per.capita.current.USD <- as.numeric(Data$GDP.per.capita.current.USD)
  Data$Individuals.using.the.Internet..of.population <- as.numeric(Data$Individuals.using.the.Internet..of.population)
  Data$Inflation.GDP.deflator.annual. <- as.numeric(Data$Inflation.GDP.deflator.annual.)
  Data$Labor.force.total <- as.numeric(Data$Labor.force.total)
  Data$Life.expectancy.at.birth.total.years <- as.numeric(Data$Life.expectancy.at.birth.total.years)
  Data$Mortality.rate.infant.per.1000.live.births <- as.numeric(Data$Mortality.rate.infant.per.1000.live.births)
  Data$People.using.at.least.basic.drinking.water.services..of.population <- as.numeric(Data$People.using.at.least.basic.drinking.water.services..of.population)
  Data$Population.total <- as.numeric(Data$Population.total)
  Data$School.enrollment.primary..gross <- as.numeric(Data$School.enrollment.primary..gross)
  Data$Suicide.mortality.rate.per.100000.population <- as.numeric(Data$Suicide.mortality.rate.per.100000.population)
  }

### Análise exploratótia por variável
#Summary
summary(Data$Adjusted.net.national.income.per.capita.current.USD)
summary(Data$Birth.rate.crude.per.1000.people)
summary(Data$Current.health.expenditure..of.GDP)
summary(Data$Foreign.direct.investment.net.BoP.current.USD)
summary(Data$GDP.current.USD)
summary(Data$GDP.per.capita.current.USD)
summary(Data$Individuals.using.the.Internet..of.population)
summary(Data$Inflation.GDP.deflator.annual.)
summary(Data$Labor.force.total)
summary(Data$Life.expectancy.at.birth.total.years)
summary(Data$Mortality.rate.infant.per.1000.live.births)
summary(Data$People.using.at.least.basic.drinking.water.services..of.population)
summary(Data$Population.total)
summary(Data$School.enrollment.primary..gross)
summary(Data$Suicide.mortality.rate.per.100000.population)

#Boxplots
for (col in names(Data)) {
  if (is.numeric(Data[[col]])) {
    boxplot(Data[[col]], main = paste("Boxplot de", col), col = "lightblue", horizontal = TRUE)
  }
}

# Conversão colunas de percentagem para decimais
Data$Current.health.expenditure..of.GDP <- Data$Current.health.expenditure..of.GDP / 100
Data$Individuals.using.the.Internet..of.population <- Data$Individuals.using.the.Internet..of.population / 100
Data$People.using.at.least.basic.drinking.water.services..of.population <- Data$People.using.at.least.basic.drinking.water.services..of.population / 100
Data$Birth.rate.crude.per.1000.people <- Data$Birth.rate.crude.per.1000.people / 1000
Data$Mortality.rate.infant.per.1000.live.births <- Data$Mortality.rate.infant.per.1000.live.births / 1000
Data$Suicide.mortality.rate.per.100000.population <- Data$Suicide.mortality.rate.per.100000.population / 100000
Data$School.enrollment.primary..gross <- Data$School.enrollment.primary..gross / 100

# Encontra os Outliers e substitui por NA
for (col in names(Data)) {
  if (is.numeric(Data[[col]])) {
    quartiles <- quantile(Data[[col]], probs=c(.25, .75), na.rm = TRUE)
    
    iQRange <- IQR(Data[[col]], na.rm = TRUE)
    lowerLimit <- quartiles[1] - 1.5 * iQRange
    upperLimit <- quartiles[2] + 1.5 * iQRange
    
    # Substituir outliers por NA diretamente na coluna existente
    Data[[col]][Data[[col]] < lowerLimit | Data[[col]] > upperLimit] <- NA
  }
}

str(Data)

#Realizar a imputação KNN 
Data_kNN <- kNN(Data, k = 5, imp_var = FALSE)

###Normalizar###
# Identificar as colunas numéricas
colunas_numericas <- sapply(Data_kNN, is.numeric)

# Normalizar manualmente todas as colunas numéricas para o intervalo [0, 1] (método Min-Max)
normalize_minmax <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Aplicar a função de normalização a todas as colunas numéricas
Data_kNN[, colunas_numericas] <- lapply(Data_kNN[, colunas_numericas], normalize_minmax)

## Alteração do nome das colunas por siglas para facilitar a visualização na matriz de correlação
#Ver nomes
names(Data_kNN)

#Criar dicionário para consulta
Dicionario <- data.frame(
  Nomes = c("Adjusted.net.national.income.per.capita.current.USD","Birth.rate.crude.per.1000.people","Current.health.expenditure.%.of.GDP","Foreign.direct.investment.net.BoP.current.USD","GDP.current.USD","GDP.per.capita.current.USD","Individuals.using.the.Internet.%.of.population","Inflation.GDP.deflator.annual.","Labor.force.total","Life.expectancy.at.birth.total.years","Mortality.rate.infant.per.1000.live.births","People.using.at.least.basic.drinking.water.services.%.of.population","Population.total","School.enrollment.primary..gross","Suicide.mortality.rate.per.100000.population"),
  Siglas = c("ANNIPC","BR","HE","FDI","GDP","GDPPC","IUI","IGDA","LF","LEAB","MRI","BDWS","POP","SEP","SMR")
)

#Alterar os nomes para as Siglas assignadas no dicionário
{
colnames(Data_kNN)[colnames(Data_kNN) == "Adjusted.net.national.income.per.capita.current.USD"] <- "ANNIPC"
colnames(Data_kNN)[colnames(Data_kNN) == "Birth.rate.crude.per.1000.people"] <- "BR"
colnames(Data_kNN)[colnames(Data_kNN) == "Current.health.expenditure..of.GDP"] <- "HE"
colnames(Data_kNN)[colnames(Data_kNN) == "Foreign.direct.investment.net.BoP.current.USD"] <- "FDI"
colnames(Data_kNN)[colnames(Data_kNN) == "GDP.current.USD"] <- "GDP"
colnames(Data_kNN)[colnames(Data_kNN) == "GDP.per.capita.current.USD"] <- "GDPPC"
colnames(Data_kNN)[colnames(Data_kNN) == "Individuals.using.the.Internet..of.population"] <- "IUI"
colnames(Data_kNN)[colnames(Data_kNN) == "Inflation.GDP.deflator.annual."] <- "IGDA"
colnames(Data_kNN)[colnames(Data_kNN) == "Labor.force.total"] <- "LF"
colnames(Data_kNN)[colnames(Data_kNN) == "Life.expectancy.at.birth.total.years"] <- "LEAB"
colnames(Data_kNN)[colnames(Data_kNN) == "Mortality.rate.infant.per.1000.live.births"] <- "MRI"
colnames(Data_kNN)[colnames(Data_kNN) == "People.using.at.least.basic.drinking.water.services..of.population"] <- "BDWS"
colnames(Data_kNN)[colnames(Data_kNN) == "Population.total"] <- "POP"
colnames(Data_kNN)[colnames(Data_kNN) == "School.enrollment.primary..gross"] <- "SEP"
colnames(Data_kNN)[colnames(Data_kNN) == "Suicide.mortality.rate.per.100000.population"] <- "SMR"
}


####ACP####
#Matriz de covariância
Data_numeric <- Data_kNN[,3:17]
summary(Data_numeric)
round(var(Data_numeric),2)

#scatterplot
pairs(Data_numeric, pch = 19, lower.panel = NULL)

# Adicionar uma legenda
legend("bottomleft", legend = colnames(Data_numeric), col = 1:ncol(Data_numeric), pch = 19, title = "Variáveis")

#matriz de correlação
correlation <- cor(Data_numeric)
round(correlation,3)

#corrplot
par(oma = c(2, 2, 2, 2)) # space around for text
corrplot.mixed(correlation, 
               order = "hclust", #order of variables
               tl.pos = "lt", #text left + top
               upper = "ellipse",
               tl.cex = 0.5)

#Bartlett test - valor p baixo -> rejeita-se H0 -> há diferenças entre a matriz de correlação observada e a matriz de identidade 
cortest.bartlett(correlation)

#KMO - Overall e individual MSA com valores relativamente altos
KMO(correlation)






#### PCA´s ####





pc15 <- principal(Data_numeric, nfactors=15, rotate="none", scores=TRUE) 

#Eigenvalues - Variâncias dos componentes principais 
round(pc15$values,3)
#CRITÉRIO DE KAISER - reter os quatro primeiros componentes principais, já que são os únicos com valores maiores que 1

#Screeplot - Encontrar o cotovelo
plot(pc15$values, type = "b", main = "Scree plot dos Dados", xlab = "Number of PC", ylab = "Eigenvalue")
#De acordo com este gráfico, o "cotovelo" está no 3, por isso reter as 2 primeiras componentes principais

#Olhar para a cumulative Var
pc15$loadings
#critério: cumulative var>= 0.6 --> logo, escolher 3 componentes principais 

#Há incertezas entre 3 ou 4 PC´s, logo, ver as comunalidades de cada solução

#Extrair 4 PC
pc4 <- principal(Data_numeric, nfactors=4, rotate="none")
pc4
#Rodar 4 componentes usando varimax
pc4r <- principal(Data_numeric, nfactors=4, rotate="varimax")
pc4r$loadings
#Comunalidades
round(pc4r$communality,4)

#Extrair 3 PC
pc3 <- principal(Data_numeric, nfactors=3, rotate="none")
pc3
#Rodar 4 componentes usando varimax
pc3r <- principal(Data_numeric, nfactors=3, rotate="varimax")
pc3r$loadings
#Comunalidades
round(pc3r$communality,3)


#Escolher 4 PC´s por causa de critério de kaiser e porque com 3 PC perde-se demasiada informação(comprovado pelas comunalidades)


#calcular os resultados
pc4sc <- principal(Data_numeric, nfactors=4, rotate="none", scores=TRUE) 
round(pc4sc$scores,4)


#Escolher nomes para cada PCA


#Aidcionar resultados aos dados como novas variáveis
Data_kNN$Progresso.Socioenonomico <- pc4sc$scores[,1]
Data_kNN$Dinamica.Populacional.e.Mercado.de.Trabalho <- pc4sc$scores[,2]
Data_kNN$Iniciativa.economica <- pc4sc$scores[,3]
Data_kNN$Saude.mental.e.Educacao <- pc4sc$scores[,4]





#### Clusters ####

#Adicionar variáveis perfil
Profile <- read.csv("PerfilCountry.csv", sep = ",", dec = ".", header = T)

#Juntar as variáveis perfil ao data frame 
Data_kNN <- merge(Data_kNN, Profile, by = "Country.Name", all.x = TRUE)

#Criar dataframe com as componentes principais e os nomes dos países
DataPCA<-Data_kNN[,c(2,18,19,20,21)] #componentes principais
DataProfile<-Data_kNN[,c(22,23,24,25)] #variaveis profile

###Normalizar###
# Identificar as colunas numéricas da PCA
colunas_numericasPCA <- sapply(DataPCA, is.numeric)
# Aplicar a função de normalização a todas as colunas numéricas da PCA
DataPCA[, colunas_numericasPCA] <- lapply(DataPCA[, colunas_numericasPCA], normalize_minmax)

countries<- DataPCA[,1] # Criar dataframe com apenas os nomes dos países, pode ser útil para representar gráficos
DataPCA<- DataPCA[,-1] # retirar o nome dos países do dataframe







### Clustering Hierárquico ###



##Método Ward 
demodist<-dist(DataPCA)
hclust_demow <- hclust(demodist,method='ward.D2')
plot(hclust_demow,label = countries,hang=-1)

#Cortar o dendrograma em 4 clusters - método ward
groups.k4<-cutree(hclust_demow, k=4) 
rect.hclust(hclust_demow, k=4, border="red") 
aggregate(DataPCA,list(groups.k4), mean)
table(groups.k4)
#Silhouette
plot(silhouette(groups.k4,demodist)) 
#average silhouette 0.18

#Cortar o dendrograma em 5 clusters - método ward
plot(hclust_demow,label = countries, hang=-1)
groups.k5<-cutree(hclust_demow, k=5) 
rect.hclust(hclust_demow, k=5, border="red") 
aggregate(DataPCA,list(groups.k5), mean)
table(groups.k5)
#Silhouette
plot(silhouette(groups.k5,demodist)) 
#average silhouette 0.20



##Método Complete Linkage
hclust_democ<-hclust(demodist)
plot(hclust_democ,label = countries, hang=-1)

#Cortar o dendrograma em 4 clusters - método complete linkage
groups.k4c<-cutree(hclust_democ, k=4)
rect.hclust(hclust_democ, k=4, border="red") 
aggregate(DataPCA,list(groups.k4c), mean)
table(groups.k4c)
table(groups.k4,groups.k4c)
#Silhouette
plot(silhouette(groups.k4c,demodist)) 
#average silhouette 0.14

#Cortar o dendrograma em 5 clusters - método complete linkage
groups.k5c<-cutree(hclust_democ, k=5)
rect.hclust(hclust_democ, k=5, border="red") 
aggregate(DataPCA,list(groups.k5c), mean)
table(groups.k5c) #composição dos clusters
table(groups.k5,groups.k5c)
#Silhouette
plot(silhouette(groups.k5c,demodist)) 
#average silhouette 0.15


#Os valores da Silhouette obtidos indicam que o método Ward é melhor







### Partitioning approach ###


## Scree plot - método "cotovelo"
set.seed(123)
wssplot <- function(xx, nc=15, seed=1234){
  wss <- (nrow(DataPCA)-1)*sum(apply(DataPCA,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(xx, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}
wssplot(DataPCA, nc=10) 
#Não é possível concluir nada

#Gráfico com a melhor silhouette por número de cluster
set.seed(123)
fviz_nbclust(DataPCA,kmeans, method = "silhouette") # 4 ou 5 clusters


## K-Means (com 4 clusters)
kmeans.k4 <- kmeans(DataPCA, 4,nstart=100) 
attributes(kmeans.k4)
kmeans.k4$centers
kmeans.k4$cluster
kmeans.k4$size
table(groups.k4,kmeans.k4$cluster)
#Silhouette
plot(silhouette(kmeans.k4$cluster,demodist))
#Silhouette = 0.24


## K-Means (com 5 clusters)
kmeans.k5 <- kmeans(DataPCA, 5,nstart=100) 
attributes(kmeans.k5)
kmeans.k5$centers
kmeans.k5$cluster
kmeans.k5$size
table(kmeans.k5$cluster)
table(groups.k5,kmeans.k5$cluster)
#Silhouette
plot(silhouette(kmeans.k5$cluster,demodist))
#Silhouette = 0.25, melhor valor que obtivemos



## PAM clustering
#É uma versão robusta do k-means, que agrupa os dados em torno dos medoides.

#PAM para 4 clusters
set.seed(123)
pam.k4 <- pam(DataPCA,4)
pam.k4$medoids
pam.k4$clustering
table(kmeans.k4$cluster,pam.k4$clustering)
plot(silhouette(pam.k4$clustering,demodist)) 
# Silhouette = 0.21

#PAM para 5 clusters
set.seed(123)
pam.k5 <- pam(DataPCA,5)
pam.k5$medoids
pam.k5$clustering
table(kmeans.k5$cluster,pam.k5$clustering)
plot(silhouette(pam.k5$clustering,demodist)) 
# Silhouette = 0.20








### Clustering Probabilístico###


## Gaussian mixture model (GMM) 
# Número ótimo de componentes
BIC <- mclustBIC(DataPCA, G = 1:15)
plot(BIC)
results <- Mclust(DataPCA, x = BIC)
summary(results, parameters = TRUE) #Conclui-se que o númnero ótimo de componentes é 3

#Mapas de classificação e incerteza com 3 clusters
plot(results, what = "classification")
plot(results, what = "uncertainty")




##Como o número ótimo de componentes no GMM é 3, vai-se fazer o método k-means (porque foi o método que obteu uma silhoette mais elevada = 0.25) com 3 componentes

#Método Ward para 3 clusters
groups.k3<-cutree(hclust_demow, k=3)
rect.hclust(hclust_demow, k=3, border="red") 
aggregate(DataPCA,list(groups.k3), mean)
table(groups.k3)

#Método k-means para 3 clusters
kmeans.k3 <- kmeans(DataPCA, 3,nstart=100) 
attributes(kmeans.k3)
kmeans.k3$centers
kmeans.k3$cluster
kmeans.k3$size
table(groups.k3,kmeans.k3$cluster)
#Silhouette
plot(silhouette(kmeans.k3$cluster,demodist))
# Sillhouete = 0.23, o que significa que perdeu eficácia relativamente ao melhor método(k-means com 5 componentes),
# mas em retorno ganhou simplicidade, porque passou de 5 componentes para apenas 3


#Adicionar os clusters ao dataset para perceber a qual cluster os paises pertenciam (apenas para facilitar interpretacao)
Data_kNN$Cluster1 <- kmeans.k3$cluster==1
Data_kNN$Cluster2 <- kmeans.k3$cluster==2
Data_kNN$Cluster3 <- kmeans.k3$cluster==3


#Analisar padrões e dar nome aos clusters com base nas variáveis profile

#Cluster 1
filtered_data1 <- subset(Data_kNN, Cluster1 == TRUE)
table(filtered_data1$Predominant.Religion)
table(filtered_data1$Continent)
table(filtered_data1$World.Climate.Data)
table(filtered_data1$Language)

##% de dados por coluna
{
  #Continent
  # Contagem de frequência
  cont_freq_cont <- table(filtered_data1$Continent)
  # Porcentagem de cada valor
  C1_cont_percent <- prop.table(cont_freq_cont) * 100
  C1_cont_percent <- data.frame(Continent = names(C1_cont_percent), Percentage = C1_cont_percent)
  
  #Predominant.Religion.x
  # Contagem de frequência
  cont_freq_PR <- table(filtered_data1$Predominant.Religion)
  # Porcentagem de cada valor
  C1_PR_percent <- prop.table(cont_freq_PR) * 100
  C1_PR_percent <- data.frame(PredReligion = names(C1_PR_percent), Percentage = C1_PR_percent)
  
  #Language.x
  # Contagem de frequência
  cont_freq_L <- table(filtered_data1$Language)
  # Porcentagem de cada valor
  C1_LANG_percent <- prop.table(cont_freq_L) * 100
  C1_LANG_percent <- data.frame(Language = names(C1_LANG_percent), Percentage = C1_LANG_percent)
  
  #World.Climate.Data.x
  # Contagem de frequência
  cont_freq_WCD <- table(filtered_data1$World.Climate.Data)
  # Porcentagem de cada valor
  C1_WCD_percent <- prop.table(cont_freq_WCD) * 100
  C1_WCD_percent <- data.frame(WorldClimate = names(C1_WCD_percent), Percentage = C1_WCD_percent)
}

#Cluster 2
filtered_data2 <- subset(Data_kNN, Cluster2 == TRUE)
table(filtered_data2$Predominant.Religion)
table(filtered_data2$Continent)
table(filtered_data2$World.Climate.Data)
table(filtered_data2$Language)

##% de dados por coluna
{
  #Continent
  # Contagem de frequência
  cont_freq_cont2 <- table(filtered_data2$Continent)
  # Porcentagem de cada valor
  C2_cont_percent <- prop.table(cont_freq_cont2) * 100
  C2_cont_percent <- data.frame(Continent = names(C2_cont_percent), Percentage = C2_cont_percent)
  
  #Predominant.Religion.x
  # Contagem de frequência
  cont_freq_PR2 <- table(filtered_data2$Predominant.Religion)
  # Porcentagem de cada valor
  C2_PR_percent <- prop.table(cont_freq_PR2) * 100
  C2_PR_percent <- data.frame(PredReligion = names(C2_PR_percent), Percentage = C2_PR_percent)
  
  #Language.x
  # Contagem de frequência
  cont_freq_L2 <- table(filtered_data2$Language)
  # Porcentagem de cada valor
  C2_LANG_percent <- prop.table(cont_freq_L2) * 100
  C2_LANG_percent <- data.frame(Language = names(C2_LANG_percent), Percentage = C2_LANG_percent)
  
  #World.Climate.Data.x
  # Contagem de frequência
  cont_freq_WCD2 <- table(filtered_data2$World.Climate.Data)
  # Porcentagem de cada valor
  C2_WCD_percent <- prop.table(cont_freq_WCD2) * 100
  C2_WCD_percent <- data.frame(WorldClimate = names(C2_WCD_percent), Percentage = C2_WCD_percent)
}

#Cluster 3
filtered_data3 <- subset(Data_kNN, Cluster3 == TRUE)
table(filtered_data3$Predominant.Religion)
table(filtered_data3$Continent)
table(filtered_data3$World.Climate.Data)
table(filtered_data3$Language)

##% de dados por coluna
{
  #Continent
  # Contagem de frequência
  cont_freq_cont3 <- table(filtered_data3$Continent)
  # Porcentagem de cada valor
  C3_cont_percent <- prop.table(cont_freq_cont3) * 100
  C3_cont_percent <- data.frame(Continent = names(C3_cont_percent), Percentage = C3_cont_percent)
  
  #Predominant.Religion.x
  # Contagem de frequência
  cont_freq_PR3 <- table(filtered_data3$Predominant.Religion)
  # Porcentagem de cada valor
  C3_PR_percent <- prop.table(cont_freq_PR3) * 100
  C3_PR_percent <- data.frame(PredReligion = names(C3_PR_percent), Percentage = C3_PR_percent)
  
  #Language.x
  # Contagem de frequência
  cont_freq_L3 <- table(filtered_data3$Language)
  # Porcentagem de cada valor
  C3_LANG_percent <- prop.table(cont_freq_L3) * 100
  C3_LANG_percent <- data.frame(Language = names(C3_LANG_percent), Percentage = C3_LANG_percent)
  
  #World.Climate.Data.x
  # Contagem de frequência
  cont_freq_WCD3 <- table(filtered_data3$World.Climate.Data)
  # Porcentagem de cada valor
  C3_WCD_percent <- prop.table(cont_freq_WCD3) * 100
  C3_WCD_percent <- data.frame(WorldClimate = names(C3_WCD_percent), Percentage = C3_WCD_percent)
}




### Tabela final ###
result <- aggregate(DataPCA, list(kmeans.k3$cluster), mean)
